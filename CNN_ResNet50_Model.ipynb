{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcbc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "# 사용자 지정 디렉터리 반복\n",
    "import numpy as np\n",
    "# six.moves.range를 사용하고 목록이 필요한 경우 목록 호출 내에서 호출을 래핑\n",
    "from six.moves import range\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import DirectoryIterator, ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# regular expression for splitting by whitespace\n",
    "splitter = re.compile(\"\\s+\")\n",
    "base_path = './dataset/img/img'\n",
    "\n",
    "def process_folders():\n",
    "    # 주석파일 전처리    \n",
    "    with open('./dataset/list_eval_partition.txt', 'r') as eval_partition_file:\n",
    "        list_eval_partition = [line.rstrip('/n') for line in eval_partition_file][2:]\n",
    "        list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
    "        list_all = [(v[0][4:], v[0].split('/')[1].split('_')[-1], v[1]) for v in list_eval_partition]\n",
    "\n",
    "    # Put each image into the relevant folder in train/test/validation folder\n",
    "    for element in list_all:\n",
    "        if not os.path.exists(os.path.join(base_path, element[2])):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(base_path, element[2]))\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        if not os.path.exists(os.path.join(os.path.join(base_path, element[2]), element[1])):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(os.path.join(base_path, element[2]), element[1]))\n",
    "            except Exception as er:\n",
    "                continue\n",
    "        if not os.path.exists(os.path.join(os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1])),\n",
    "                              element[0].split('/')[0])):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1])),\n",
    "                     element[0].split('/')[0]))\n",
    "            except Exception as err:\n",
    "                continue\n",
    "        try:\n",
    "            shutil.move(os.path.join(base_path, element[0]),\n",
    "                        os.path.join(os.path.join(os.path.join(base_path, element[2]), element[1]), element[0]))\n",
    "        except Exception as errr:\n",
    "            continue\n",
    "process_folders()\n",
    "\n",
    "\n",
    "\n",
    "def get_dict_bboxes():   \n",
    "    \n",
    "    with open('./dataset/list_category_img.txt', 'r') as category_img_file, \\\n",
    "            open('./dataset/list_eval_partition.txt', 'r') as eval_partition_file, \\\n",
    "            open('./dataset/list_bbox.txt', 'r') as bbox_file:\n",
    "                \n",
    "        list_category_img = [line.rstrip('\\n') for line in category_img_file][2:]\n",
    "        list_eval_partition = [line.rstrip('\\n') for line in eval_partition_file][2:]\n",
    "        list_bbox = [line.rstrip('\\n') for line in bbox_file][2:]\n",
    "\n",
    "        list_category_img = [splitter.split(line) for line in list_category_img]\n",
    "        list_eval_partition = [splitter.split(line) for line in list_eval_partition]\n",
    "        list_bbox = [splitter.split(line) for line in list_bbox]\n",
    "\n",
    "        list_all = [(k[0][4:], k[0].split('/')[1].split('_')[-1], v[1], (int(b[1]), int(b[2]), int(b[3]), int(b[4])))\n",
    "                    for k, v, b in zip(list_category_img, list_eval_partition, list_bbox)]\n",
    "        #print(list(list_all[0]))\n",
    "        #list_all.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # TODO : delete!!\n",
    "        # list_all = list_all[:10]# \n",
    "        \n",
    "        #print(len(dict_train))\n",
    "        dict_train = create_dict_bboxes(list_all)\n",
    "        dict_val = create_dict_bboxes(list_all, split='val')\n",
    "        dict_test = create_dict_bboxes(list_all, split='test')\n",
    "    return dict_train, dict_val, dict_test\n",
    "    \n",
    "    \n",
    "#Keras에서 사전 학습 된 ResNet50 모델\n",
    "\n",
    "model_resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "for layer in model_resnet.layers[:-12]:\n",
    "    # 6 - 12 - 18 have been tried. 12 is the best.\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = model_resnet.output\n",
    "x = Dense(512, activation='elu', kernel_regularizer=l2(0.001))(x)\n",
    "y = Dense(46, activation='softmax', name='img')(x)\n",
    "\n",
    "x_bbox = model_resnet.output \n",
    "x_bbox = Dense (512, activation = 'relu', kernel_regularizer = l2 (0.001))(x_bbox) \n",
    "x_bbox = Dense (128, activation = 'relu', kernel_regularizer = l2 (0.001))(x_bbox) \n",
    "bbox = Dense (4, kernel_initializer = 'normal', name = 'bbox')(x_bbox)\n",
    "\n",
    "final_model = Model(inputs=model_resnet.input,\n",
    "                    outputs=[y, bbox])\n",
    "\n",
    "print(final_model.summary())\n",
    "\n",
    "opt = SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "\n",
    "final_model.compile(optimizer=opt,\n",
    "                    loss={'img': 'categorical_crossentropy',\n",
    "                          'bbox': 'mean_squared_error'},\n",
    "                    metrics={'img': ['accuracy', 'top_k_categorical_accuracy'], # default: top-5\n",
    "                             'bbox': ['mse']})\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=30.,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "class DirectoryIteratorWithBoundingBoxes(DirectoryIterator):\n",
    "    def __init__(self, directory, image_data_generator, bounding_boxes: dict = None, target_size=(256, 256),\n",
    "                 color_mode: str = 'rgb', classes=None, class_mode: str = 'categorical', batch_size: int = 32,\n",
    "                 shuffle: bool = True, seed=None, data_format=None, save_to_dir=None,\n",
    "                 save_prefix: str = '', save_format: str = 'jpeg', follow_links: bool = False):\n",
    "        super().__init__(directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size,\n",
    "                         shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links)\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        locations = np.zeros((len(batch_x),) + (4,), dtype=K.floatx())\n",
    "\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            fname = self.filenames[j]\n",
    "            img = image.load_img(os.path.join(self.directory, fname),\n",
    "                                 grayscale=grayscale,\n",
    "                                 target_size=self.target_size)\n",
    "            x = image.img_to_array(img, data_format=self.data_format)\n",
    "            x = self.image_data_generator.random_transform(x)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "            \n",
    "            \n",
    "            if self.bounding_boxes is not None:\n",
    "             \n",
    "                bounding_box = self.bounding_boxes[fname]\n",
    "                locations[i] = np.asarray(\n",
    "                    [bounding_box['x1'], bounding_box['y1'], bounding_box['x2'], bounding_box['y2']],\n",
    "                    dtype=K.floatx())\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        # build batch of labels\n",
    "        if self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), 46), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "\n",
    "        if self.bounding_boxes is not None:\n",
    "            return batch_x, [batch_y, locations]\n",
    "        else:\n",
    "            return batch_x, batch_y\n",
    "        \n",
    "        \n",
    "dict_train, dict_val, dict_test = get_dict_bboxes()\n",
    "\n",
    "train_iterator = DirectoryIteratorWithBoundingBoxes(\"./dataset/img/img/train\", train_datagen, bounding_boxes=dict_train, target_size=(200, 200))\n",
    "test_iterator = DirectoryIteratorWithBoundingBoxes(\"./dataset/img/img/val\", test_datagen, bounding_boxes=dict_val,target_size=(200, 200))\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               patience=12,\n",
    "                               factor=0.5,\n",
    "                               verbose=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n",
    "early_stopper = EarlyStopping(monitor='val_loss',\n",
    "                              patience=30,\n",
    "                              verbose=1)\n",
    "checkpoint = ModelCheckpoint('./models/model.h5')\n",
    "\n",
    "def custom_generator(iterator):\n",
    "    while True:\n",
    "        batch_x, batch_y = iterator.next()\n",
    "        yield (batch_x, batch_y)\n",
    "        \n",
    "final_model.fit_generator(custom_generator(train_iterator),\n",
    "                          steps_per_epoch=2000,\n",
    "                          epochs=10, validation_data=custom_generator(test_iterator),\n",
    "                          validation_steps=200,\n",
    "                          verbose=2,\n",
    "                          shuffle=True,\n",
    "                          callbacks=[lr_reducer, checkpoint, early_stopper, tensorboard],\n",
    "                          workers=12)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_iterator = DirectoryIteratorWithBoundingBoxes(\"./dataset/img/img/test\", test_datagen, bounding_boxes=dict_test, target_size=(200, 200))\n",
    "scores = final_model.evaluate_generator(custom_generator(test_iterator), steps=2000)\n",
    "\n",
    "print('Multi target loss: ' + str(scores[0]))\n",
    "print('Image loss: ' + str(scores[1]))\n",
    "print('Bounding boxes loss: ' + str(scores[2]))\n",
    "print('Image accuracy: ' + str(scores[3]))\n",
    "print('Top-5 image accuracy: ' + str(scores[4]))\n",
    "print('Bounding boxes error: ' + str(scores[5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata",
   "language": "python",
   "name": "pydata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
